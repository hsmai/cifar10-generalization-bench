{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# SETUP 및 CIFAR-10 DATASET 준비"
   ],
   "metadata": {
    "id": "aSjeHntCNwdy"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "#0. Setup\n",
    "\n",
    "import os, time, random\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_everything(42)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "g9rkSEVuSGef",
    "outputId": "22e984b7-6823-4f6a-e9c0-74935d9666e7"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#1. CIFAR-10 load & split\n",
    "\n",
    "DATA_ROOT = \"./data\"\n",
    "\n",
    "CIFAR10_MEAN = (0.4914, 0.4822, 0.4465)\n",
    "CIFAR10_STD  = (0.2470, 0.2435, 0.2616)\n",
    "\n",
    "\n",
    "tf_noaug = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(CIFAR10_MEAN, CIFAR10_STD),\n",
    "])\n",
    "\n",
    "tf_test = tf_noaug\n",
    "\n",
    "def make_splits(n, val_ratio=0.1, seed=42):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    idx = np.arange(n)\n",
    "    rng.shuffle(idx)\n",
    "    val_size = int(n * val_ratio)\n",
    "    val_idx = idx[:val_size].tolist()\n",
    "    train_idx = idx[val_size:].tolist()\n",
    "    return train_idx, val_idx\n",
    "\n",
    "class CIFARSplit(Dataset):\n",
    "\n",
    "    def __init__(self, base_dataset, indices, transform):\n",
    "        self.base = base_dataset\n",
    "        self.indices = indices\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        idx = self.indices[i]\n",
    "        img, y = self.base[idx]\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        return img, y\n",
    "\n",
    "# base dataset\n",
    "base_train_raw = datasets.CIFAR10(root=DATA_ROOT, train=True,  download=True, transform=None)\n",
    "base_test_raw  = datasets.CIFAR10(root=DATA_ROOT, train=False, download=True, transform=None)\n",
    "\n",
    "train_idx, val_idx = make_splits(len(base_train_raw), val_ratio=0.1, seed=42)\n",
    "\n",
    "train_full = CIFARSplit(base_train_raw, train_idx, transform=tf_noaug)\n",
    "val_set    = CIFARSplit(base_train_raw, val_idx,   transform=tf_noaug)\n",
    "test_set   = CIFARSplit(base_test_raw,  list(range(len(base_test_raw))), transform=tf_test)\n",
    "\n",
    "print(\"full train:\", len(train_full), \"val:\", len(val_set), \"test:\", len(test_set))\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BWAhVF3RZzX7",
    "outputId": "11935d9c-eb92-49d4-d825-a74606518ee0"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# ALEXNET IMPLEMENTATION & OVERFITTING"
   ],
   "metadata": {
    "id": "psajuKYVXVtn"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "#2. AlexNet style model implementation\n",
    "\n",
    "class AlexNetCIFAR(nn.Module):\n",
    "    def __init__(self, num_classes=10, dropout_p=0.0):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 3, 1, 1), nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "\n",
    "            nn.Conv2d(64, 192, 3, 1, 1), nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "\n",
    "            nn.Conv2d(192, 384, 3, 1, 1), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(384, 256, 3, 1, 1), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, 3, 1, 1), nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(p=dropout_p),\n",
    "            nn.Linear(256 * 4 * 4, 1024), nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=dropout_p),\n",
    "            nn.Linear(1024, 512), nn.ReLU(inplace=True),\n",
    "            nn.Linear(512, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        return self.classifier(x)\n",
    "\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
    "        if m.bias is not None:\n",
    "            nn.init.zeros_(m.bias)\n",
    "    elif isinstance(m, nn.Linear):\n",
    "        nn.init.kaiming_normal_(m.weight, nonlinearity=\"relu\")\n",
    "        if m.bias is not None:\n",
    "            nn.init.zeros_(m.bias)\n",
    "\n",
    "model = AlexNetCIFAR(dropout_p=0.0).to(device)\n",
    "model.apply(init_weights)\n",
    "\n",
    "x, y = next(iter(DataLoader(train_full, batch_size=8, shuffle=True)))\n",
    "logits = model(x.to(device))\n",
    "print(\"logits shape:\", logits.shape)\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OSDsv6bmZzUh",
    "outputId": "b66acb28-d9e5-400a-e063-a2491ef8970d"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#3. Overfitting\n",
    "\n",
    "OVERFIT_SUBSET = 1000\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 200\n",
    "LR = 0.05\n",
    "MOMENTUM = 0.9\n",
    "\n",
    "# 1) train subset\n",
    "train_overfit = torch.utils.data.Subset(\n",
    "    train_full,\n",
    "    list(range(min(OVERFIT_SUBSET, len(train_full))))\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_overfit, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n",
    "val_loader   = DataLoader(val_set,       batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "# 2) model with no regularization\n",
    "model = AlexNetCIFAR(dropout_p=0.0).to(device)\n",
    "model.apply(init_weights)\n",
    "\n",
    "# 3) loss / optimizer\n",
    "crit = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LR, momentum=MOMENTUM, weight_decay=0.0)\n",
    "\n",
    "def accuracy(logits, y):\n",
    "    return (logits.argmax(dim=1) == y).float().mean().item()\n",
    "\n",
    "history = {\"train_loss\": [], \"train_acc\": [], \"val_loss\": [], \"val_acc\": []}\n",
    "\n",
    "\n",
    "OUT_OVER_DIR = \"./outputs/overfit\"\n",
    "os.makedirs(OUT_OVER_DIR, exist_ok=True)\n",
    "OVERFIT_CKPT_PATH = os.path.join(OUT_OVER_DIR, \"overfit.pth\")\n",
    "\n",
    "# early stop\n",
    "TARGET_TRAIN_ACC = 0.99\n",
    "GAP_THRESHOLD = 0.15\n",
    "PATIENCE = 2\n",
    "hit = 0\n",
    "\n",
    "epoch_times_over = []\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    t0 = time.time()\n",
    "\n",
    "    #train\n",
    "    model.train()\n",
    "    tr_loss, tr_acc, n = 0.0, 0.0, 0\n",
    "    for x, y in train_loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        logits = model(x)\n",
    "        loss = crit(logits, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        bs = x.size(0)\n",
    "        tr_loss += loss.item() * bs\n",
    "        tr_acc  += accuracy(logits, y) * bs\n",
    "        n += bs\n",
    "\n",
    "    tr_loss /= n\n",
    "    tr_acc  /= n\n",
    "\n",
    "    #val\n",
    "    model.eval()\n",
    "    va_loss, va_acc, n = 0.0, 0.0, 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in val_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            logits = model(x)\n",
    "            loss = crit(logits, y)\n",
    "\n",
    "            bs = x.size(0)\n",
    "            va_loss += loss.item() * bs\n",
    "            va_acc  += accuracy(logits, y) * bs\n",
    "            n += bs\n",
    "\n",
    "    va_loss /= n\n",
    "    va_acc  /= n\n",
    "\n",
    "    history[\"train_loss\"].append(tr_loss)\n",
    "    history[\"train_acc\"].append(tr_acc)\n",
    "    history[\"val_loss\"].append(va_loss)\n",
    "    history[\"val_acc\"].append(va_acc)\n",
    "\n",
    "    if epoch == 1 or epoch % 10 == 0 or epoch == EPOCHS:\n",
    "        print(f\"Epoch {epoch:03d}/{EPOCHS} | \"\n",
    "              f\"train {tr_loss:.4f}/{tr_acc:.4f} | val {va_loss:.4f}/{va_acc:.4f} | \"\n",
    "              f\"{time.time()-t0:.1f}s\")\n",
    "\n",
    "    epoch_times_over.append(time.time() - t0)\n",
    "\n",
    "    # early stop check\n",
    "    # a. train acc가 충분히 높으면 바로 종료\n",
    "    cond_train_fit = (tr_acc >= TARGET_TRAIN_ACC)\n",
    "\n",
    "    # b. train, val의 gap이 커야함\n",
    "    cond_gap = ((tr_acc - va_acc) >= GAP_THRESHOLD)\n",
    "\n",
    "\n",
    "    if cond_train_fit and cond_gap:\n",
    "        hit += 1\n",
    "    else:\n",
    "        hit = 0\n",
    "\n",
    "    # train이 거의 완벽하면 바로 종료\n",
    "    if cond_train_fit:\n",
    "        print(f\"Early stop: train acc reached {tr_acc:.4f} at epoch {epoch}. Saving checkpoint...\")\n",
    "        torch.save({\n",
    "            \"model\": model.state_dict(),\n",
    "            \"meta\": {\n",
    "                \"subset\": OVERFIT_SUBSET,\n",
    "                \"epoch\": epoch,\n",
    "                \"train_acc\": tr_acc,\n",
    "                \"val_acc\": va_acc,\n",
    "                \"stop_reason\": \"train_acc_threshold\"\n",
    "            }\n",
    "        }, OVERFIT_CKPT_PATH)\n",
    "        break\n",
    "\n",
    "if not os.path.exists(OVERFIT_CKPT_PATH):\n",
    "    print(\"Early stop not triggered. Saving final checkpoint...\")\n",
    "    torch.save({\n",
    "        \"model\": model.state_dict(),\n",
    "        \"meta\": {\n",
    "            \"subset\": OVERFIT_SUBSET,\n",
    "            \"epoch\": EPOCHS,\n",
    "            \"train_acc\": history[\"train_acc\"][-1],\n",
    "            \"val_acc\": history[\"val_acc\"][-1]\n",
    "        }\n",
    "    }, OVERFIT_CKPT_PATH)\n",
    "\n",
    "history_overfit = history\n",
    "\n",
    "print(\"Final train acc:\", history[\"train_acc\"][-1], \"Final val acc:\", history[\"val_acc\"][-1])\n",
    "print(\"Saved overfit checkpoint to:\", OVERFIT_CKPT_PATH)\n",
    "\n",
    "\n",
    "test_loader_over = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "model.eval()\n",
    "te_loss, te_acc, n = 0.0, 0.0, 0\n",
    "with torch.no_grad():\n",
    "    for x, y in test_loader_over:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        logits = model(x)\n",
    "        loss = crit(logits, y)\n",
    "\n",
    "        bs = x.size(0)\n",
    "        te_loss += loss.item() * bs\n",
    "        te_acc  += accuracy(logits, y) * bs\n",
    "        n += bs\n",
    "\n",
    "te_loss /= n\n",
    "te_acc  /= n\n",
    "print(f\"[overfit] test loss = {te_loss:.4f} | test acc = {te_acc:.4f}\")\n",
    "\n",
    "\n",
    "# 마지막 acc 출력\n",
    "print(\"Final train acc:\", history[\"train_acc\"][-1], \"Final val acc:\", history[\"val_acc\"][-1])\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(history[\"train_loss\"], label=\"train_loss\")\n",
    "plt.plot(history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.legend(); plt.title(\"Overfit: loss\"); plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(history[\"train_acc\"], label=\"train_acc\")\n",
    "plt.plot(history[\"val_acc\"], label=\"val_acc\")\n",
    "plt.legend(); plt.title(\"Overfit: acc\"); plt.show()\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "chrZFugBZzRX",
    "outputId": "fb0937a9-3f3b-464e-de04-1cb9dc09ceff"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# REGULARIZATION & DATA AUGMENTATION"
   ],
   "metadata": {
    "id": "E629noa7XiG0"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "#4. Regularization & Data Augmentation\n",
    "\n",
    "# a. augmentation transform\n",
    "tf_aug = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),   # aug #1\n",
    "    transforms.RandomHorizontalFlip(),      # aug #2\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(CIFAR10_MEAN, CIFAR10_STD),\n",
    "])\n",
    "\n",
    "# b. FULL train set을 augmentation transform으로 다시 만들기\n",
    "train_full_aug = CIFARSplit(base_train_raw, train_idx, transform=tf_aug)\n",
    "\n",
    "BATCH_SIZE_REG = 128\n",
    "EPOCHS_REG = 100\n",
    "LR_REG = 0.01\n",
    "MOMENTUM_REG = 0.9\n",
    "\n",
    "DROPOUT_P = 0.5        # regularization #1\n",
    "WEIGHT_DECAY = 5e-4    # regularization #2\n",
    "\n",
    "train_loader_reg = DataLoader(train_full_aug, batch_size=BATCH_SIZE_REG, shuffle=True,\n",
    "                              num_workers=2, pin_memory=True)\n",
    "val_loader_reg   = DataLoader(val_set,       batch_size=BATCH_SIZE_REG, shuffle=False,\n",
    "                              num_workers=2, pin_memory=True)\n",
    "test_loader      = DataLoader(test_set,      batch_size=BATCH_SIZE_REG, shuffle=False,\n",
    "                              num_workers=2, pin_memory=True)\n",
    "\n",
    "# 3) 모델 구성 (dropout ON)\n",
    "model_reg = AlexNetCIFAR(dropout_p=DROPOUT_P).to(device)\n",
    "model_reg.apply(init_weights)\n",
    "\n",
    "crit = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model_reg.parameters(), lr=LR_REG, momentum=MOMENTUM_REG, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "def accuracy(logits, y):\n",
    "    return (logits.argmax(dim=1) == y).float().mean().item()\n",
    "\n",
    "def eval_loop(model, loader):\n",
    "    model.eval()\n",
    "    total_loss, total_acc, n = 0.0, 0.0, 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            logits = model(x)\n",
    "            loss = crit(logits, y)\n",
    "            bs = x.size(0)\n",
    "            total_loss += loss.item() * bs\n",
    "            total_acc  += accuracy(logits, y) * bs\n",
    "            n += bs\n",
    "    return total_loss / n, total_acc / n\n",
    "\n",
    "history_reg = {\"train_loss\": [], \"train_acc\": [], \"val_loss\": [], \"val_acc\": []}\n",
    "best_val_acc = -1.0\n",
    "\n",
    "OUT_REG_DIR = \"./outputs/regularized\"\n",
    "os.makedirs(OUT_REG_DIR, exist_ok=True)\n",
    "REG_CKPT_PATH = os.path.join(OUT_REG_DIR, \"regularized.pth\")\n",
    "\n",
    "epoch_times_reg = []\n",
    "\n",
    "\n",
    "for epoch in range(1, EPOCHS_REG + 1):\n",
    "    t0 = time.time()\n",
    "\n",
    "    # train\n",
    "    model_reg.train()\n",
    "    tr_loss, tr_acc, n = 0.0, 0.0, 0\n",
    "    for x, y in train_loader_reg:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        logits = model_reg(x)\n",
    "        loss = crit(logits, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        bs = x.size(0)\n",
    "        tr_loss += loss.item() * bs\n",
    "        tr_acc  += accuracy(logits, y) * bs\n",
    "        n += bs\n",
    "\n",
    "    tr_loss /= n\n",
    "    tr_acc  /= n\n",
    "\n",
    "    # val\n",
    "    va_loss, va_acc = eval_loop(model_reg, val_loader_reg)\n",
    "\n",
    "    history_reg[\"train_loss\"].append(tr_loss)\n",
    "    history_reg[\"train_acc\"].append(tr_acc)\n",
    "    history_reg[\"val_loss\"].append(va_loss)\n",
    "    history_reg[\"val_acc\"].append(va_acc)\n",
    "\n",
    "    # best 저장\n",
    "    if va_acc > best_val_acc:\n",
    "        best_val_acc = va_acc\n",
    "        torch.save({\n",
    "            \"model\": model_reg.state_dict(),\n",
    "            \"meta\": {\n",
    "                \"dropout_p\": DROPOUT_P,\n",
    "                \"weight_decay\": WEIGHT_DECAY,\n",
    "                \"aug\": [\"RandomCrop(32,pad=4)\", \"RandomHorizontalFlip\"],\n",
    "                \"epoch\": epoch,\n",
    "                \"best_val_acc\": best_val_acc\n",
    "            }\n",
    "        }, REG_CKPT_PATH)\n",
    "\n",
    "    if epoch == 1 or epoch % 10 == 0 or epoch == EPOCHS_REG:\n",
    "        print(f\"[regularized] Epoch {epoch:03d}/{EPOCHS_REG} | \"\n",
    "              f\"train {tr_loss:.4f}/{tr_acc:.4f} | val {va_loss:.4f}/{va_acc:.4f} | \"\n",
    "              f\"{time.time()-t0:.1f}s\")\n",
    "\n",
    "\n",
    "    epoch_times_reg.append(time.time() - t0)\n",
    "\n",
    "\n",
    "# test 평가\n",
    "ckpt = torch.load(REG_CKPT_PATH, map_location=device)\n",
    "model_reg.load_state_dict(ckpt[\"model\"])\n",
    "test_loss_reg, test_acc_reg = eval_loop(model_reg, test_loader)\n",
    "\n",
    "print(f\"[regularized] BEST val acc = {best_val_acc:.4f} | test acc = {test_acc_reg:.4f}\")\n",
    "\n",
    "with open(os.path.join(OUT_REG_DIR, \"log.json\"), \"w\") as f:\n",
    "    json.dump({**history_reg, \"best_val_acc\": best_val_acc, \"test_loss\": test_loss_reg, \"test_acc\": test_acc_reg}, f, indent=2)\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LdBez2e6ZzN5",
    "outputId": "76bc8c06-9cf5-47de-ae37-486fa4423098"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# regularized model 결과 시각화\n",
    "\n",
    "import os, json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if \"history_reg\" not in globals() or history_reg is None or len(history_reg.get(\"train_loss\", [])) == 0:\n",
    "    log_path = os.path.join(\"./outputs/regularized\", \"log.json\")\n",
    "    assert os.path.exists(log_path), f\"log.json not found: {log_path}\"\n",
    "    with open(log_path, \"r\") as f:\n",
    "        history_reg = json.load(f)\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "\n",
    "# loss\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(history_reg[\"train_loss\"], label=\"train_loss\")\n",
    "plt.plot(history_reg[\"val_loss\"],   label=\"val_loss\")\n",
    "plt.legend()\n",
    "plt.title(\"Regularized: loss\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "\n",
    "# acc\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(history_reg[\"train_acc\"], label=\"train_acc\")\n",
    "plt.plot(history_reg[\"val_acc\"],   label=\"val_acc\")\n",
    "plt.legend()\n",
    "plt.title(\"Regularized: acc\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 407
    },
    "id": "ClMAPEAPxGJa",
    "outputId": "e4a3dea0-f376-45df-fe09-906f84a1048e"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# GENERALIZATION ANALYSIS"
   ],
   "metadata": {
    "id": "jxtIhfqTXruw"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "#5-1. Quantitative comparison\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "h_over = history_overfit\n",
    "h_reg  = history_reg\n",
    "\n",
    "\n",
    "OVERFIT_CKPT_PATH = \"./outputs/overfit/overfit.pth\"\n",
    "REG_CKPT_PATH     = \"./outputs/regularized/regularized.pth\"\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_acc(model, loader):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        pred = model(x).argmax(dim=1)\n",
    "        correct += (pred == y).sum().item()\n",
    "        total += y.numel()\n",
    "    return correct / total\n",
    "\n",
    "def load_model_from_ckpt(ckpt_path, dropout_p):\n",
    "    ckpt = torch.load(ckpt_path, map_location=device)\n",
    "    state = ckpt[\"model\"] if isinstance(ckpt, dict) and \"model\" in ckpt else ckpt\n",
    "    m = AlexNetCIFAR(dropout_p=dropout_p).to(device)\n",
    "    m.load_state_dict(state)\n",
    "    return m\n",
    "\n",
    "\n",
    "m_over = load_model_from_ckpt(OVERFIT_CKPT_PATH, dropout_p=0.0)\n",
    "m_reg  = load_model_from_ckpt(REG_CKPT_PATH,     dropout_p=DROPOUT_P)  # 학습 설정과 동일\n",
    "\n",
    "# same evaluation sets\n",
    "EVAL_BS = 256\n",
    "\n",
    "train_eval_loader_over = DataLoader(train_overfit, batch_size=EVAL_BS, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "train_eval_loader = DataLoader(train_full, batch_size=EVAL_BS, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "val_eval_loader   = DataLoader(val_set,    batch_size=EVAL_BS, shuffle=False, num_workers=2, pin_memory=True)\n",
    "test_eval_loader  = DataLoader(test_set,   batch_size=EVAL_BS, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "# accuracies\n",
    "final_train_over = eval_acc(m_over, train_eval_loader_over)  # ✅ (수정) 1000장 subset 기준\n",
    "final_val_over   = eval_acc(m_over, val_eval_loader)\n",
    "final_test_over  = eval_acc(m_over, test_eval_loader)\n",
    "\n",
    "final_train_reg = eval_acc(m_reg, train_eval_loader)\n",
    "final_val_reg   = eval_acc(m_reg, val_eval_loader)\n",
    "final_test_reg  = eval_acc(m_reg, test_eval_loader)\n",
    "\n",
    "\n",
    "test_acc_over, test_acc_reg = final_test_over, final_test_reg\n",
    "\n",
    "rows = [\n",
    "    [\"Overfit\",     final_train_over, final_val_over, final_test_over, final_train_over - final_val_over],\n",
    "    [\"Regularized\", final_train_reg,  final_val_reg,  final_test_reg,  final_train_reg  - final_val_reg],\n",
    "]\n",
    "\n",
    "print(\"=== Final Accuracy Summary (SAME val/test; Overfit-train is on its 1000-sample subset) ===\")\n",
    "print(f\"{'Model':<12} | {'Train Acc':>9} | {'Val Acc':>7} | {'Test Acc':>8} | {'Train-Val Gap':>12}\")\n",
    "print(\"-\"*78)\n",
    "for r in rows:\n",
    "    print(f\"{r[0]:<12} | {r[1]:9.4f} | {r[2]:7.4f} | {r[3]:8.4f} | {r[4]:12.4f}\")\n",
    "\n",
    "\n",
    "# curves\n",
    "m = min(len(h_over[\"train_loss\"]), len(h_reg[\"train_loss\"]))\n",
    "xs = np.arange(1, m+1)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(xs, h_over[\"train_loss\"][:m], label=\"overfit_train_loss\")\n",
    "plt.plot(xs, h_over[\"val_loss\"][:m],   label=\"overfit_val_loss\")\n",
    "plt.plot(xs, h_reg[\"train_loss\"][:m],  label=\"reg_train_loss\")\n",
    "plt.plot(xs, h_reg[\"val_loss\"][:m],    label=\"reg_val_loss\")\n",
    "plt.xlabel(\"epoch\"); plt.ylabel(\"loss\")\n",
    "plt.title(\"Train vs Val Loss (Both Models on same plot)\")\n",
    "plt.legend(); plt.tight_layout(); plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(xs, h_over[\"train_acc\"][:m], label=\"overfit_train_acc\")\n",
    "plt.plot(xs, h_over[\"val_acc\"][:m],   label=\"overfit_val_acc\")\n",
    "plt.plot(xs, h_reg[\"train_acc\"][:m],  label=\"reg_train_acc\")\n",
    "plt.plot(xs, h_reg[\"val_acc\"][:m],    label=\"reg_val_acc\")\n",
    "plt.xlabel(\"epoch\"); plt.ylabel(\"accuracy\")\n",
    "plt.title(\"Train vs Val Accuracy (Both Models on same plot)\")\n",
    "plt.legend(); plt.tight_layout(); plt.show()\n",
    "\n",
    "gap_over = np.array(h_over[\"train_acc\"][:m]) - np.array(h_over[\"val_acc\"][:m])\n",
    "gap_reg  = np.array(h_reg[\"train_acc\"][:m])  - np.array(h_reg[\"val_acc\"][:m])\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(xs, gap_over, label=\"overfit_gap(train-val)\")\n",
    "plt.plot(xs, gap_reg,  label=\"reg_gap(train-val)\")\n",
    "plt.xlabel(\"epoch\"); plt.ylabel(\"train-val acc gap\")\n",
    "plt.title(\"Overfitting evidence: Train-Val Accuracy Gap\")\n",
    "plt.legend(); plt.tight_layout(); plt.show()\n",
    "\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "PJaRqh5f7s2t",
    "outputId": "06d5ef75-76a7-43e9-cdd0-f215f5954375"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#5-2. Qualitative comparison\n",
    "\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "if \"m_over\" not in globals():\n",
    "    m_over = load_model_from_ckpt(OVERFIT_CKPT_PATH, dropout_p=0.0)\n",
    "if \"m_reg\" not in globals():\n",
    "    m_reg  = load_model_from_ckpt(REG_CKPT_PATH, dropout_p=DROPOUT_P)\n",
    "\n",
    "\n",
    "CLASS_NAMES = [\"airplane\",\"automobile\",\"bird\",\"cat\",\"deer\",\"dog\",\"frog\",\"horse\",\"ship\",\"truck\"]\n",
    "\n",
    "@torch.no_grad()\n",
    "def collect_correct_incorrect(model, base_test_raw, tf_test, n_each=8):\n",
    "    model.eval()\n",
    "    correct, incorrect = [], []\n",
    "    for idx in range(len(base_test_raw)):\n",
    "        img_pil, y = base_test_raw[idx]\n",
    "        x = tf_test(img_pil).unsqueeze(0).to(device)\n",
    "        pred = model(x).argmax(dim=1).item()\n",
    "\n",
    "        if pred == y and len(correct) < n_each:\n",
    "            correct.append((idx, img_pil, y, pred))\n",
    "        if pred != y and len(incorrect) < n_each:\n",
    "            incorrect.append((idx, img_pil, y, pred))\n",
    "\n",
    "        if len(correct) >= n_each and len(incorrect) >= n_each:\n",
    "            break\n",
    "    return correct, incorrect\n",
    "\n",
    "def show_grid(samples, title):\n",
    "    cols = 4\n",
    "    rows = math.ceil(len(samples)/cols)\n",
    "    plt.figure(figsize=(cols*3, rows*3))\n",
    "    for i, (idx, img, y, pred) in enumerate(samples):\n",
    "        plt.subplot(rows, cols, i+1)\n",
    "        plt.imshow(img)\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(f\"GT:{CLASS_NAMES[y]}\\nPred:{CLASS_NAMES[pred]}\\nidx={idx}\", fontsize=9)\n",
    "    plt.suptitle(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# a. 정/오분류 시각화\n",
    "correct_over, incorrect_over = collect_correct_incorrect(m_over, base_test_raw, tf_test, n_each=8)\n",
    "correct_reg,  incorrect_reg  = collect_correct_incorrect(m_reg,  base_test_raw, tf_test, n_each=8)\n",
    "\n",
    "show_grid(correct_over,   \"Overfit model: Correctly classified (test)\")\n",
    "show_grid(incorrect_over, \"Overfit model: Misclassified (test)\")\n",
    "show_grid(correct_reg,    \"Regularized model: Correctly classified (test)\")\n",
    "show_grid(incorrect_reg,  \"Regularized model: Misclassified (test)\")\n",
    "\n",
    "# b. Train time 비교\n",
    "if \"epoch_times_over\" in globals() and \"epoch_times_reg\" in globals():\n",
    "    print(\"=== Training Time Comparison ===\")\n",
    "    print(f\"Overfit:     total {sum(epoch_times_over):.1f}s | per-epoch {np.mean(epoch_times_over):.2f}s\")\n",
    "    print(f\"Regularized: total {sum(epoch_times_reg):.1f}s | per-epoch {np.mean(epoch_times_reg):.2f}s\")\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(epoch_times_over, label=\"overfit_epoch_time\")\n",
    "    plt.plot(epoch_times_reg,  label=\"reg_epoch_time\")\n",
    "    plt.xlabel(\"epoch\"); plt.ylabel(\"seconds\")\n",
    "    plt.title(\"Training Time per Epoch (if recorded)\")\n",
    "    plt.legend(); plt.tight_layout(); plt.show()\n",
    "else:\n",
    "    print(\"NOTE: epoch_times_over/epoch_times_reg not found. (Optional) If you want, I can show how to log epoch times during training.\")\n",
    "\n",
    "# c. overfit 근거 시각화\n",
    "m = min(len(h_over[\"val_loss\"]), len(h_reg[\"val_loss\"]))\n",
    "xs = np.arange(1, m+1)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(xs, h_over[\"val_loss\"][:m], label=\"overfit_val_loss\")\n",
    "plt.plot(xs, h_reg[\"val_loss\"][:m],  label=\"reg_val_loss\")\n",
    "plt.xlabel(\"epoch\"); plt.ylabel(\"val loss\")\n",
    "plt.title(\"Overfitting evidence: Validation Loss (Overfit vs Regularized)\")\n",
    "plt.legend(); plt.tight_layout(); plt.show()\n",
    "\n",
    "# d. training dynamics 시각화: 개선 속도 비교 (val_acc가 epoch마다 얼마나 빨리 증가/감소하는지)\n",
    "val_acc_over = np.array(h_over[\"val_acc\"][:m])\n",
    "val_acc_reg  = np.array(h_reg[\"val_acc\"][:m])\n",
    "\n",
    "delta_over = np.diff(val_acc_over, prepend=val_acc_over[0])\n",
    "delta_reg  = np.diff(val_acc_reg,  prepend=val_acc_reg[0])\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(xs, delta_over, label=\"overfit Δval_acc\")\n",
    "plt.plot(xs, delta_reg,  label=\"reg Δval_acc\")\n",
    "plt.xlabel(\"epoch\"); plt.ylabel(\"change in val_acc\")\n",
    "plt.title(\"Training dynamics: Val accuracy improvement speed (Δval_acc)\")\n",
    "plt.legend(); plt.tight_layout(); plt.show()\n",
    "\n",
    "# e. 최종 일반화 성능 시각화: bar chart (train/val/test acc)\n",
    "train_accs = [final_train_over, final_train_reg]\n",
    "val_accs   = [final_val_over,   final_val_reg]\n",
    "test_accs  = [test_acc_over,    test_acc_reg]\n",
    "labels = [\"Overfit\", \"Regularized\"]\n",
    "x = np.arange(len(labels))\n",
    "w = 0.25\n",
    "\n",
    "plt.figure()\n",
    "plt.bar(x - w, train_accs, width=w, label=\"train\")\n",
    "plt.bar(x,     val_accs,   width=w, label=\"val\")\n",
    "plt.bar(x + w, test_accs,  width=w, label=\"test\")\n",
    "plt.xticks(x, labels)\n",
    "plt.ylim(0, 1.0)\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.title(\"Final Generalization Performance (Train/Val/Test)\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "epDT9YIV8Oeo",
    "outputId": "e9a44abd-a898-4934-8582-13278099b88e"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# 6. 모델 저장\n",
    "\n",
    "import os, torch\n",
    "\n",
    "SUB_DIR = \"./submission_models\"\n",
    "os.makedirs(SUB_DIR, exist_ok=True)\n",
    "\n",
    "OVERFIT_SRC = \"./outputs/overfit/overfit.pth\"\n",
    "REG_SRC     = \"./outputs/regularized/regularized.pth\"\n",
    "\n",
    "def save_state_dict_only(src_path, dst_path):\n",
    "    ckpt = torch.load(src_path, map_location=\"cpu\")\n",
    "    state = ckpt[\"model\"] if isinstance(ckpt, dict) and \"model\" in ckpt else ckpt\n",
    "    torch.save(state, dst_path)\n",
    "\n",
    "save_state_dict_only(OVERFIT_SRC, os.path.join(SUB_DIR, \"overfit_state_dict.pth\"))\n",
    "save_state_dict_only(REG_SRC,     os.path.join(SUB_DIR, \"regularized_state_dict.pth\"))\n",
    "\n",
    "print(\"✅ Saved clean model parameters (state_dict only) to:\", SUB_DIR)\n",
    "print(\" -\", os.path.join(SUB_DIR, \"overfit_state_dict.pth\"))\n",
    "print(\" -\", os.path.join(SUB_DIR, \"regularized_state_dict.pth\"))\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xsi4fcC4H8PC",
    "outputId": "eeff48f2-b1c9-4820-ff2e-6594ae805689"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}